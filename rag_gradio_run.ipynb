{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb0ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q uninstall -y langchain langchain-core langchain-community langchain-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f369d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"openai==1.52.0\" \"langchain==0.2.16\" \"langchain-community\" \"langchain-openai\" \"sentence-transformers==3.0.1\" \"qdrant-client==1.9.1\" \"faiss-cpu==1.8.0.post1\" \"beautifulsoup4==4.12.3\" \"lxml==5.3.0\" \"requests==2.32.3\" \"pypdf==4.3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d501d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "import re, requests\n",
    "from typing import List\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948533dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SITES_HTML = [\n",
    "    (\"BIDS\",     \"https://bids-specification.readthedocs.io/en/stable/\"),\n",
    "    (\"fMRIPrep\", \"https://fmriprep.readthedocs.io/en/stable/\"),\n",
    "]\n",
    "SITES_PDF = [\n",
    "    \n",
    "    (\"MRtrix\", \"https://media.readthedocs.org/pdf/mrtrix/latest/mrtrix.pdf\"),\n",
    "    (\"SPM12\",  \"https://www.fil.ion.ucl.ac.uk/spm/doc/spm12_manual.pdf\"),\n",
    "\n",
    "]\n",
    "MAX_DEPTH = 1  # keep tiny & fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a90250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BIDS] pages: 1\n",
      "[fMRIPrep] pages: 1\n",
      "[MRtrix] pdf pages: 378\n",
      "[SPM12] pdf pages: 533\n",
      "TOTAL docs: 913\n"
     ]
    }
   ],
   "source": [
    "def clean_html_text(html: str) -> str:\n",
    "    t = Soup(html, \"html.parser\").get_text(\" \", strip=True)\n",
    "    return re.sub(r\"\\s{2,}\", \" \", t)\n",
    "\n",
    "def load_web_docs() -> List:\n",
    "    docs = []\n",
    "    for name, url in SITES_HTML:\n",
    "        loader = RecursiveUrlLoader(url=url, max_depth=MAX_DEPTH,\n",
    "                                    extractor=lambda x: clean_html_text(x))\n",
    "        d = loader.load()\n",
    "        for x in d:\n",
    "            x.metadata[\"tool\"] = name\n",
    "        print(f\"[{name}] pages:\", len(d))\n",
    "        docs.extend(d)\n",
    "    return docs\n",
    "\n",
    "def load_pdf_docs() -> List:\n",
    "    docs = []\n",
    "    for name, url in SITES_PDF:\n",
    "        pdf_bytes = requests.get(url, timeout=120).content\n",
    "        with open(f\"{name}.pdf\", \"wb\") as f: f.write(pdf_bytes)\n",
    "        ld = PyPDFLoader(f\"{name}.pdf\").load()\n",
    "        for x in ld:\n",
    "            x.metadata[\"tool\"] = name\n",
    "            x.metadata[\"source\"] = f\"{name}.pdf\"\n",
    "        print(f\"[{name}] pdf pages:\", len(ld))\n",
    "        docs.extend(ld)\n",
    "    return docs\n",
    "\n",
    "web_docs = load_web_docs()\n",
    "pdf_docs = load_pdf_docs()\n",
    "all_docs = web_docs + pdf_docs\n",
    "print(\"TOTAL docs:\", len(all_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18ef4798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL chunks: 2794\n"
     ]
    }
   ],
   "source": [
    "\n",
    "section_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n## \", \"\\n### \", \"\\n# \", \"\\n\\n\", \"\\n\", \" \"],\n",
    "    chunk_size=1800, chunk_overlap=150\n",
    ")\n",
    "\n",
    "def sentenceish_pack(text, size=1100, overlap=150):\n",
    "    sents = re.split(r'(?<=[\\.\\?\\!])\\s+', text.strip())\n",
    "    cur, out = \"\", []\n",
    "    for s in sents:\n",
    "        if len(cur) + len(s) + 1 <= size:\n",
    "            cur = (cur + \" \" + s).strip()\n",
    "        else:\n",
    "            if cur: out.append(cur)\n",
    "            cur = s\n",
    "    if cur: out.append(cur)\n",
    "    stitched = []\n",
    "    for i, ch in enumerate(out):\n",
    "        tail = out[i-1][-overlap:] if i>0 else \"\"\n",
    "        stitched.append((tail + \" \" + ch).strip())\n",
    "    return stitched\n",
    "\n",
    "def make_chunks(docs: List):\n",
    "    sections = section_splitter.split_documents(docs)\n",
    "    chunks = []\n",
    "    for d in sections:\n",
    "        for piece in sentenceish_pack(d.page_content):\n",
    "            nd = d.copy()\n",
    "            nd.page_content = piece\n",
    "            chunks.append(nd)\n",
    "    return chunks\n",
    "\n",
    "chunks = make_chunks(all_docs)\n",
    "print(\"TOTAL chunks:\", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b659c5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sahan\\AppData\\Local\\Temp\\ipykernel_31336\\743173267.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "c:\\Users\\sahan\\Documents\\neural_agent\\.venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings... (this may take a few minutes)\n",
      "✓ Embeddings created and retriever ready!\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Initialize E5 embeddings (small & efficient)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-small-v2\",\n",
    "    model_kwargs={'device': 'cpu'},  # Use 'cuda' if GPU available\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# Create FAISS vector store from chunks\n",
    "print(\"Creating embeddings... (this may take a few minutes)\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # Return top 4 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"✓ Embeddings created and retriever ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52c3b3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install httpx==0.27.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d29af94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "SYSTEM = \"\"\"You are a precise RAG assistant for BIDS/fMRIPrep/MRtrix/SPM.\n",
    "Use ONLY the provided context. If missing, say you don't know.\n",
    "Be concise (<=6 sentences). Add a final 'Sources:' line with URLs/filenames.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "def format_docs(docs, max_chars=2000):\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        src = d.metadata.get(\"source\") or d.metadata.get(\"url\") or \"unknown\"\n",
    "        txt = d.page_content[:max_chars]\n",
    "        if len(d.page_content) > max_chars:\n",
    "            txt = txt.rsplit(\". \", 1)[0] + \".\"\n",
    "        out.append(f\"{txt}\\n(Source: {src})\")\n",
    "    return \"\\n\\n---\\n\\n\".join(out)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def ask(q: str):\n",
    "    # E5 works best with \"query: \" prefix for queries\n",
    "    return chain.invoke(\"query: \" + q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1bae68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-proj-ew\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"OPENAI_API_KEY\")[:10])  # Shows first 10 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "542b6e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fMRIPrep produces several outputs for BOLD runs, including preprocessed functional data, confound regressors, and visual reports. The preprocessed data is resampled onto standard spaces, such as EPI sampled to FreeSurfer surfaces and HCP Grayordinates. Additionally, fMRIPrep generates confound regressors that can be used in subsequent analyses, along with a description of these confounds. Visual reports are also created to facilitate quality control and interpretation of the preprocessing results. \n",
      "\n",
      "Sources: https://fmriprep.readthedocs.io/en/stable/\n"
     ]
    }
   ],
   "source": [
    "# For MiniLM (no prefix)\n",
    "print(chain.invoke(\"What outputs does fMRIPrep produce for BOLD runs?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18c21ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd9c7c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://ddf0a2a58ea3a957fd.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ddf0a2a58ea3a957fd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "USE_E5 = True  # True only if you used E5 embeddings\n",
    "\n",
    "custom_css = \"\"\"\n",
    "#title {font-size: 28px; font-weight: 800; letter-spacing:.2px}\n",
    "#subtitle {opacity:.85; margin-top:-8px}\n",
    ".gr-textbox textarea {font-size:16px}\n",
    "\"\"\"\n",
    "\n",
    "def answer_fn(q, hist):\n",
    "    if not q.strip():\n",
    "        return q, gr.update(), hist\n",
    "    q_in = f\"query: {q}\" if USE_E5 else q\n",
    "    ans = chain.invoke(q_in)\n",
    "\n",
    "    new_block = f\"**Q:** {q}\\n\\n{ans}\"\n",
    "    hist = [new_block] + hist          # <-- put newest at the TOP\n",
    "\n",
    "    return \"\", \"\\n\\n---\\n\\n\".join(hist), hist\n",
    "\n",
    "with gr.Blocks(title=\"Neuro-Docs RAG\", css=custom_css, theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"<div id='title'> Neuro-Docs RAG</div>\"\n",
    "                \"<div id='subtitle'>BIDS · fMRIPrep · MRtrix · SPM</div>\")\n",
    "    state = gr.State([])  # keeps the running Q/A markdown\n",
    "\n",
    "    # Input row (on top)\n",
    "    with gr.Group():\n",
    "        qbox = gr.Textbox(label=None, placeholder=\"Ask about BIDS / fMRIPrep…\",\n",
    "                          lines=2, autofocus=True)\n",
    "        send = gr.Button(\"Ask\", variant=\"primary\")\n",
    "\n",
    "    # Answers area (below input)\n",
    "    out_md = gr.Markdown(label=None)\n",
    "\n",
    "    qbox.submit(answer_fn, [qbox, state], [qbox, out_md, state])\n",
    "    send.click(answer_fn, [qbox, state], [qbox, out_md, state])\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c337c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
